{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import expon, reciprocal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import deap\n",
    "import skopt\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import joblib\n",
    "import optuna \n",
    "\n",
    "def preprocess_data(file_path, is_train=True):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert binary categorical features to 0 and 1\n",
    "    binary_features = ['CryoSleep', 'VIP']\n",
    "    df[binary_features] = df[binary_features].astype(bool).astype(int)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df['TotalSpending'] = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)\n",
    "    \n",
    "    # Conditionally set spending-related features to 0 for passengers in cryosleep\n",
    "    spending_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df.loc[df['CryoSleep'] == 1, spending_features] = 0\n",
    "    \n",
    "    # Create interaction features\n",
    "    df['HomePlanet_TotalSpending'] = df['HomePlanet'].astype(str) + '_' + df['TotalSpending'].astype(str)\n",
    "    df['Destination_TotalSpending'] = df['Destination'].astype(str) + '_' + df['TotalSpending'].astype(str)\n",
    "    \n",
    "    # Extract components from 'Cabin'\n",
    "    if 'Cabin' in df.columns:\n",
    "        df[['Cabin_Deck', 'Cabin_Number', 'Cabin_Side']] = df['Cabin'].str.split('/', expand=True)\n",
    "        df['Cabin_Side'] = df['Cabin_Side'].map({'P': 1, 'S': 0})\n",
    "        df['Cabin_Number'] = pd.to_numeric(df['Cabin_Number'], errors='coerce')\n",
    "        df.drop('Cabin', axis=1, inplace=True)\n",
    "    \n",
    "    # One-hot encode multi-category features\n",
    "    multi_cat_features = ['HomePlanet', 'Destination']\n",
    "    if is_train:\n",
    "        one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        encoded_features = one_hot_encoder.fit_transform(df[multi_cat_features])\n",
    "        encoded_feature_names = one_hot_encoder.get_feature_names_out(multi_cat_features)\n",
    "        joblib.dump(one_hot_encoder, 'one_hot_encoder.pkl')\n",
    "    else:\n",
    "        one_hot_encoder = joblib.load('one_hot_encoder.pkl')\n",
    "        encoded_features = one_hot_encoder.transform(df[multi_cat_features])\n",
    "        encoded_feature_names = one_hot_encoder.get_feature_names_out(multi_cat_features)\n",
    "    \n",
    "    encoded_features_df = pd.DataFrame(encoded_features.toarray(), columns=encoded_feature_names)\n",
    "    df = pd.concat([df, encoded_features_df], axis=1)\n",
    "    df.drop(multi_cat_features, axis=1, inplace=True)\n",
    "    \n",
    "    # Imputation and Scaling\n",
    "    numeric_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Cabin_Number', 'Cabin_Side', 'TotalSpending']\n",
    "    if is_train:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        scaler = StandardScaler()\n",
    "        df[numeric_features] = imputer.fit_transform(df[numeric_features])\n",
    "        df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "        joblib.dump(imputer, 'imputer.pkl')\n",
    "        joblib.dump(scaler, 'scaler.pkl')\n",
    "    else:\n",
    "        imputer = joblib.load('imputer.pkl')\n",
    "        scaler = joblib.load('scaler.pkl')\n",
    "        df[numeric_features] = imputer.transform(df[numeric_features])\n",
    "        df[numeric_features] = scaler.transform(df[numeric_features])\n",
    "    \n",
    "    if is_train:\n",
    "        # Convert 'Transported' to integer (True=1, False=0) for modeling\n",
    "        df['Transported'] = df['Transported'].astype(int)\n",
    "        \n",
    "        # Save the list of features used for training\n",
    "        train_features = [col for col in df.columns if col not in ['PassengerId', 'Name', 'Transported', 'Cabin_Deck', 'HomePlanet_TotalSpending', 'Destination_TotalSpending']]\n",
    "        joblib.dump(train_features, 'train_features.pkl')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. PyTorch Model Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "train_df = preprocess_data('csv_files/train.csv', is_train=True)\n",
    "test_df = preprocess_data('csv_files/test.csv', is_train=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, layers, units, activation, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Reduce the number of layers and units per layer\n",
    "        layers = max(1, layers // 2)  # Ensure at least one layer\n",
    "        units = max(32, units // 2)   # Ensure a minimum number of units\n",
    "\n",
    "        for i in range(layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_shape, units))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(units, units))\n",
    "            if activation == 'relu':\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif activation == 'elu':\n",
    "                self.layers.append(nn.ELU())\n",
    "            # Increase dropout rate to prevent overfitting\n",
    "            self.layers.append(nn.Dropout(min(0.5, dropout_rate + 0.1)))\n",
    "\n",
    "        self.out = nn.Linear(units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.sigmoid(self.out(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function with Optuna and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a global variable to keep track of the best validation accuracy\n",
    "best_validation_accuracy = 0\n",
    "best_model_params = None\n",
    "def objective(trial):\n",
    "    global best_validation_accuracy, best_model_params\n",
    "    # Load and preprocess data\n",
    "    features = [col for col in train_df.columns if col not in ['PassengerId', 'Name', 'Transported', 'Cabin_Deck', 'HomePlanet_TotalSpending', 'Destination_TotalSpending']]\n",
    "    X = train_df[features].values\n",
    "    y = train_df['Transported'].values\n",
    "\n",
    "    # Split the data into training and validation sets (80/20 split)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Handling class imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_res, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_res, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Model Configuration\n",
    "    input_shape = X_train_tensor.shape[1]\n",
    "    layers = trial.suggest_int('layers', 1, 2)\n",
    "    units = trial.suggest_int('units', 32, 256)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Instantiate the model with 'tanh' activation\n",
    "    model = Net(input_shape, layers, units, 'tanh', dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # DataLoader for PyTorch\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = F.binary_cross_entropy(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_val_tensor)\n",
    "        prediction = output.round()\n",
    "        accuracy = (prediction.eq(y_val_tensor).sum() / float(y_val_tensor.size(0))).item()\n",
    "\n",
    "    if accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = accuracy\n",
    "        best_model_params = {\n",
    "            'input_shape': input_shape,\n",
    "            'layers': layers,\n",
    "            'units': units,\n",
    "            'activation': 'tanh',\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'learning_rate': learning_rate\n",
    "        }\n",
    "        torch.save(model.state_dict(), 'model_enhanced.pth')\n",
    "        \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-21 20:25:27,564] A new study created in memory with name: no-name-906132a7-03be-490b-a483-4667ca60e38e\n",
      "[I 2024-03-21 20:25:33,881] Trial 0 finished with value: 0.7734330296516418 and parameters: {'layers': 2, 'units': 105, 'dropout_rate': 0.44916995743269655, 'learning_rate': 0.0002674333559558307}. Best is trial 0 with value: 0.7734330296516418.\n",
      "[I 2024-03-21 20:25:41,688] Trial 1 finished with value: 0.7837837934494019 and parameters: {'layers': 2, 'units': 176, 'dropout_rate': 0.3857429461137464, 'learning_rate': 0.0006224958347891364}. Best is trial 1 with value: 0.7837837934494019.\n",
      "[I 2024-03-21 20:25:49,210] Trial 2 finished with value: 0.7941345572471619 and parameters: {'layers': 1, 'units': 175, 'dropout_rate': 0.288263676896526, 'learning_rate': 0.004949347635465978}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:25:55,380] Trial 3 finished with value: 0.7780333757400513 and parameters: {'layers': 2, 'units': 116, 'dropout_rate': 0.42080071817997533, 'learning_rate': 0.0002867431555451749}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:02,033] Trial 4 finished with value: 0.7878090739250183 and parameters: {'layers': 2, 'units': 92, 'dropout_rate': 0.26603722550251097, 'learning_rate': 0.007211262365901152}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:08,350] Trial 5 finished with value: 0.7826337218284607 and parameters: {'layers': 1, 'units': 68, 'dropout_rate': 0.3827497430032697, 'learning_rate': 0.002414593159612071}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:14,522] Trial 6 finished with value: 0.77400803565979 and parameters: {'layers': 1, 'units': 44, 'dropout_rate': 0.22252874484381138, 'learning_rate': 0.0002470807690221757}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:23,579] Trial 7 finished with value: 0.7832087278366089 and parameters: {'layers': 2, 'units': 252, 'dropout_rate': 0.20824098295018267, 'learning_rate': 0.003202897737779553}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:31,410] Trial 8 finished with value: 0.7814835906028748 and parameters: {'layers': 1, 'units': 135, 'dropout_rate': 0.2178098464678942, 'learning_rate': 0.0005188851901677028}. Best is trial 2 with value: 0.7941345572471619.\n",
      "[I 2024-03-21 20:26:39,034] Trial 9 finished with value: 0.7866590023040771 and parameters: {'layers': 1, 'units': 180, 'dropout_rate': 0.21736562302866305, 'learning_rate': 0.0018623426654728094}. Best is trial 2 with value: 0.7941345572471619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.7941345572471619\n",
      "Best Hyperparameters: {'layers': 1, 'units': 175, 'dropout_rate': 0.288263676896526, 'learning_rate': 0.004949347635465978}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(f\"Best Validation Accuracy: {study.best_value}\")\n",
    "print(f\"Best Hyperparameters: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optuna study\n",
    "def retrain_best_model_on_full_data(best_model_params):\n",
    "    # Combine your training and validation data\n",
    "    features = [col for col in train_df.columns if col not in ['PassengerId', 'Name', 'Transported', 'Cabin_Deck', 'HomePlanet_TotalSpending', 'Destination_TotalSpending']]\n",
    "    X = train_df[features].values\n",
    "    y = train_df['Transported'].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # Handling class imbalance on the full dataset\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    X_resampled = torch.tensor(X_res, dtype=torch.float32)\n",
    "    y_resampled = torch.tensor(y_res, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    # DataLoader for PyTorch\n",
    "    full_dataset = TensorDataset(X_resampled, y_resampled)\n",
    "    full_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Model instantiation using best model parameters\n",
    "    model = Net(\n",
    "        input_shape=best_model_params['input_shape'],\n",
    "        layers=best_model_params['layers'],\n",
    "        units=best_model_params['units'],\n",
    "        activation='tanh',\n",
    "        dropout_rate=best_model_params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_model_params['learning_rate'])\n",
    "\n",
    "    # Retrain loop on the full dataset\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_x, batch_y in full_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = F.binary_cross_entropy(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Save the retrained model\n",
    "    torch.save(model.state_dict(), 'model_enhanced_full.pth')\n",
    "if best_model_params:\n",
    "    retrain_best_model_on_full_data(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exoprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the best hyperparameters and model architecture from the Optuna study\n",
    "best_params = study.best_params\n",
    "# Assuming 'train_df' is your DataFrame and you want to exclude certain columns to get the input shape\n",
    "excluded_columns = ['PassengerId', 'Name', 'Transported', 'Cabin_Deck', 'HomePlanet_TotalSpending', 'Destination_TotalSpending']\n",
    "input_shape = len([col for col in train_df.columns if col not in excluded_columns])\n",
    "\n",
    "\n",
    "# Define the model with the best hyperparameters\n",
    "model_enhanced = Net(input_shape=input_shape, layers=best_params['layers'], units=best_params['units'],\n",
    "                     activation='tanh', dropout_rate=best_params['dropout_rate'])\n",
    "\n",
    "# Load the trained model weights\n",
    "model_enhanced.load_state_dict(torch.load('model_enhanced_full.pth'))\n",
    "model_enhanced.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# Preprocess the test data\n",
    "test_df = preprocess_data('csv_files/test.csv', is_train=False)\n",
    "\n",
    "# Select the features used by the model, ensuring they are the same as those used in training\n",
    "features = [col for col in train_df.columns if col not in ['PassengerId', 'Name', 'Transported', 'Cabin_Deck', 'HomePlanet_TotalSpending', 'Destination_TotalSpending']]\n",
    "numeric_features = [col for col in features if test_df[col].dtype != 'object']\n",
    "\n",
    "# Prepare the test data for the PyTorch model\n",
    "X_test = test_df[features].copy()\n",
    "\n",
    "# Scale the numeric features using the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_test[numeric_features] = scaler.fit_transform(X_test[numeric_features])\n",
    "\n",
    "X_test_np = X_test.values\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    y_pred_test_tensor = model_enhanced(X_test_tensor)\n",
    "    y_pred_test_proba = torch.sigmoid(y_pred_test_tensor).numpy()  # sigmoid applied if not in the model's forward method\n",
    "    y_pred_test = (y_pred_test_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Prepare the submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': y_pred_test\n",
    "})\n",
    "\n",
    "# Convert predictions back to boolean (True/False) for final submission\n",
    "submission_df['Transported'] = submission_df['Transported'].astype(bool)\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('pytorchandoptunavalac.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
